{
  "model": "llama-3.1-8b-instant",
  "dataset": "dev.json",
  "total_examples": 1034,
  "exact_match_accuracy": 0.048355899419729204,
  "execution_match_accuracy": 0.22437137330754353,
  "execution_success_rate": 0.28433268858800775,
  "error_rate": 0.7147001934235977,
  "average_time_per_query": 0.3828741900003164,
  "total_time": 395.89191246032715,
  "per_database_accuracy": {
    "concert_singer": {
      "exact_match_accuracy": 0.044444444444444446,
      "execution_match_accuracy": 0.17777777777777778,
      "execution_success_rate": 0.2,
      "mcts_usage_rate": 0.0,
      "total_examples": 45
    },
    "pets_1": {
      "exact_match_accuracy": 0.09523809523809523,
      "execution_match_accuracy": 0.21428571428571427,
      "execution_success_rate": 0.2619047619047619,
      "mcts_usage_rate": 0.0,
      "total_examples": 42
    },
    "car_1": {
      "exact_match_accuracy": 0.07608695652173914,
      "execution_match_accuracy": 0.13043478260869565,
      "execution_success_rate": 0.20652173913043478,
      "mcts_usage_rate": 0.0,
      "total_examples": 92
    },
    "flight_2": {
      "exact_match_accuracy": 0.075,
      "execution_match_accuracy": 0.5,
      "execution_success_rate": 0.5125,
      "mcts_usage_rate": 0.0,
      "total_examples": 80
    },
    "employee_hire_evaluation": {
      "exact_match_accuracy": 0.07894736842105263,
      "execution_match_accuracy": 0.18421052631578946,
      "execution_success_rate": 0.3157894736842105,
      "mcts_usage_rate": 0.0,
      "total_examples": 38
    },
    "cre_Doc_Template_Mgt": {
      "exact_match_accuracy": 0.023809523809523808,
      "execution_match_accuracy": 0.3333333333333333,
      "execution_success_rate": 0.38095238095238093,
      "mcts_usage_rate": 0.0,
      "total_examples": 84
    },
    "course_teach": {
      "exact_match_accuracy": 0.03333333333333333,
      "execution_match_accuracy": 0.3333333333333333,
      "execution_success_rate": 0.3333333333333333,
      "mcts_usage_rate": 0.0,
      "total_examples": 30
    },
    "museum_visit": {
      "exact_match_accuracy": 0.1111111111111111,
      "execution_match_accuracy": 0.16666666666666666,
      "execution_success_rate": 0.16666666666666666,
      "mcts_usage_rate": 0.0,
      "total_examples": 18
    },
    "wta_1": {
      "exact_match_accuracy": 0.06451612903225806,
      "execution_match_accuracy": 0.0967741935483871,
      "execution_success_rate": 0.16129032258064516,
      "mcts_usage_rate": 0.0,
      "total_examples": 62
    },
    "battle_death": {
      "exact_match_accuracy": 0.125,
      "execution_match_accuracy": 0.1875,
      "execution_success_rate": 0.25,
      "mcts_usage_rate": 0.0,
      "total_examples": 16
    },
    "student_transcripts_tracking": {
      "exact_match_accuracy": 0.07692307692307693,
      "execution_match_accuracy": 0.2692307692307692,
      "execution_success_rate": 0.3076923076923077,
      "mcts_usage_rate": 0.0,
      "total_examples": 78
    },
    "tvshow": {
      "exact_match_accuracy": 0.0,
      "execution_match_accuracy": 0.1774193548387097,
      "execution_success_rate": 0.2903225806451613,
      "mcts_usage_rate": 0.0,
      "total_examples": 62
    },
    "poker_player": {
      "exact_match_accuracy": 0.025,
      "execution_match_accuracy": 0.2,
      "execution_success_rate": 0.25,
      "mcts_usage_rate": 0.0,
      "total_examples": 40
    },
    "voter_1": {
      "exact_match_accuracy": 0.06666666666666667,
      "execution_match_accuracy": 0.3333333333333333,
      "execution_success_rate": 0.4,
      "mcts_usage_rate": 0.0,
      "total_examples": 15
    },
    "world_1": {
      "exact_match_accuracy": 0.025,
      "execution_match_accuracy": 0.13333333333333333,
      "execution_success_rate": 0.20833333333333334,
      "mcts_usage_rate": 0.0,
      "total_examples": 120
    },
    "orchestra": {
      "exact_match_accuracy": 0.05,
      "execution_match_accuracy": 0.2,
      "execution_success_rate": 0.225,
      "mcts_usage_rate": 0.0,
      "total_examples": 40
    },
    "network_1": {
      "exact_match_accuracy": 0.05357142857142857,
      "execution_match_accuracy": 0.30357142857142855,
      "execution_success_rate": 0.42857142857142855,
      "mcts_usage_rate": 0.0,
      "total_examples": 56
    },
    "dog_kennels": {
      "exact_match_accuracy": 0.0,
      "execution_match_accuracy": 0.13414634146341464,
      "execution_success_rate": 0.2073170731707317,
      "mcts_usage_rate": 0.0,
      "total_examples": 82
    },
    "singer": {
      "exact_match_accuracy": 0.03333333333333333,
      "execution_match_accuracy": 0.23333333333333334,
      "execution_success_rate": 0.23333333333333334,
      "mcts_usage_rate": 0.0,
      "total_examples": 30
    },
    "real_estate_properties": {
      "exact_match_accuracy": 0.0,
      "execution_match_accuracy": 0.5,
      "execution_success_rate": 0.75,
      "mcts_usage_rate": 0.0,
      "total_examples": 4
    }
  },
  "has_gold_sql": true,
  "few_shot_examples": null,
  "use_mcts": true,
  "mcts_iterations": 25,
  "mcts_usage_rate": 0.0,
  "direct_vs_mcts": {
    "direct_count": 295,
    "mcts_count": 0,
    "direct_accuracy": 0.7864406779661017,
    "mcts_accuracy": 0,
    "direct_success_rate": 0.9966101694915255,
    "mcts_success_rate": 0
  }
}