{
  "model": "llama-3.1-8b-instant",
  "dataset": "dev.json",
  "total_examples": 100,
  "exact_match_accuracy": 0.0,
  "execution_match_accuracy": 0.0,
  "execution_success_rate": 0.0,
  "error_rate": 0.05,
  "average_time_per_query": 0.08812618494033814,
  "total_time": 8.812618494033813,
  "per_database_accuracy": {
    "flight_2": {
      "exact_match_accuracy": 0.0,
      "execution_match_accuracy": 0.0,
      "execution_success_rate": 0.0,
      "total_examples": 10
    },
    "pets_1": {
      "exact_match_accuracy": 0.0,
      "execution_match_accuracy": 0.0,
      "execution_success_rate": 0.0,
      "total_examples": 8
    },
    "student_transcripts_tracking": {
      "exact_match_accuracy": 0.0,
      "execution_match_accuracy": 0.0,
      "execution_success_rate": 0.0,
      "total_examples": 6
    },
    "battle_death": {
      "exact_match_accuracy": 0.0,
      "execution_match_accuracy": 0.0,
      "execution_success_rate": 0.0,
      "total_examples": 1
    },
    "wta_1": {
      "exact_match_accuracy": 0.0,
      "execution_match_accuracy": 0.0,
      "execution_success_rate": 0.0,
      "total_examples": 6
    },
    "employee_hire_evaluation": {
      "exact_match_accuracy": 0.0,
      "execution_match_accuracy": 0.0,
      "execution_success_rate": 0.0,
      "total_examples": 5
    },
    "car_1": {
      "exact_match_accuracy": 0.0,
      "execution_match_accuracy": 0.0,
      "execution_success_rate": 0.0,
      "total_examples": 10
    },
    "network_1": {
      "exact_match_accuracy": 0.0,
      "execution_match_accuracy": 0.0,
      "execution_success_rate": 0.0,
      "total_examples": 7
    },
    "tvshow": {
      "exact_match_accuracy": 0.0,
      "execution_match_accuracy": 0.0,
      "execution_success_rate": 0.0,
      "total_examples": 7
    },
    "concert_singer": {
      "exact_match_accuracy": 0.0,
      "execution_match_accuracy": 0.0,
      "execution_success_rate": 0.0,
      "total_examples": 3
    },
    "world_1": {
      "exact_match_accuracy": 0.0,
      "execution_match_accuracy": 0.0,
      "execution_success_rate": 0.0,
      "total_examples": 8
    },
    "poker_player": {
      "exact_match_accuracy": 0.0,
      "execution_match_accuracy": 0.0,
      "execution_success_rate": 0.0,
      "total_examples": 5
    },
    "orchestra": {
      "exact_match_accuracy": 0.0,
      "execution_match_accuracy": 0.0,
      "execution_success_rate": 0.0,
      "total_examples": 5
    },
    "cre_Doc_Template_Mgt": {
      "exact_match_accuracy": 0.0,
      "execution_match_accuracy": 0.0,
      "execution_success_rate": 0.0,
      "total_examples": 9
    },
    "singer": {
      "exact_match_accuracy": 0.0,
      "execution_match_accuracy": 0.0,
      "execution_success_rate": 0.0,
      "total_examples": 3
    },
    "dog_kennels": {
      "exact_match_accuracy": 0.0,
      "execution_match_accuracy": 0.0,
      "execution_success_rate": 0.0,
      "total_examples": 4
    },
    "course_teach": {
      "exact_match_accuracy": 0.0,
      "execution_match_accuracy": 0.0,
      "execution_success_rate": 0.0,
      "total_examples": 2
    },
    "voter_1": {
      "exact_match_accuracy": 0.0,
      "execution_match_accuracy": 0.0,
      "execution_success_rate": 0.0,
      "total_examples": 1
    }
  },
  "has_gold_sql": true,
  "mcts_used": true,
  "mcts_iterations": 30,
  "mcts_usage_rate": 0.0,
  "mcts_statistics": {
    "direct_generation_count": 95,
    "mcts_generation_count": 0,
    "direct_generation_accuracy": 0.0,
    "mcts_generation_accuracy": 0,
    "direct_generation_success_rate": 0.0,
    "mcts_generation_success_rate": 0
  }
}